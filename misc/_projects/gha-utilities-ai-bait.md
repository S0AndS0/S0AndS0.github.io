---
vim: textwidth=79 nowrap
title: GitHub Actions Utilities -- AI Bait
description: GitHub Action to encourage behavior changes from AI/ML scrapers that disrespect robots.txt
layout: post
date: 2024-06-28 19:27:28 -0700
time_to_live: 1800
author: S0AndS0
tags: [ ai, cicd, docker, github, ml  ]
image: assets/images/projects/gha-utilities/ai-bait/first-code-block.png

social_comment:
  links:
    - text: LinkedIn
      href: https://www.linkedin.com/posts/s0ands0_ai-bait-github-marketplace-activity-7211932471033012225-gWXw
      title: Link to LinkedIn thread for this post

    - text: Mastodon
      href: https://mastodon.social/@S0AndS0/112686445267694435
      title: Link to Toot thread for this post

    - text: Twitter
      href: https://x.com/S0_And_S0/status/1806166792735183065
      title: Link to Tweet thread for this post

attribution:
  links:
    - text: Robb Knight -- Perplexity Is Lying about Their User Agent
      href: https://rknight.me/blog/perplexity-ai-is-lying-about-its-user-agent/
      title: Article about how certain AI scrappers ignore the `robots.txt` file and don't set proper user agent headers

    - text: Wired -- Perplexity Is a Bullshit Machine
      href: https://www.wired.com/story/perplexity-is-a-bullshit-machine/
      title: Article confirming Robb Knight's findings
---



## TLDR

Include, and modify, the following within your repository's workflow that
published to GitHub Pages

```yaml
      - name: Make something nasty for bots
        uses: gha-utilities/ai-bait@v0.0.4
        with:
          bs: 512
          count: 10000
          destination: _site/assets/ai/bait.zip
```

Basically it is a GitHub actions wrapper for the classic zip-bomb via Bash, eg.
`dd if=/dev/zero bs="<N>" count="<BLOCKS>" | zip "<DESTINATION>" -`

> Relevant snippets from `dd` manual;
>
> - `bs` → `man -P "less -p '^\s+bs='" dd` → "read and write up to BYTES bytes
>   at a time (default: 512); overrides ibs and obs"
> - `count` → `man -P "less -p '^\s+count='" dd` → "copy only N input blocks"
>
> Larger `bs` values, like say `10g`, will compress many gigs of zeros into a
> tight zip, easy on servers and rough on those that don't follow instructions.

To prevent causing issues with legitimate/authorized web-scrapers, and other
tools, be sure to keep your site's `robots.txt` up to date!  Simplest option is
to tell all robots not to access the file(s) generated by AI Bait;

```
User-agent: *
Disallow: /assets/ai/bait.zip
```


______


## Okay but why?


According to various sources web-scrapers for AI/ML training have been ignoring
site's `robots.txt` configurations, not consistently setting User-agent
headers, and resulting output often fails to provide accurate attribution.


______


## Soapbox


That last bit "fails to provide accurate attribution", to me, is the most
egregious because I authored and maintain a Liquid
[Plugin](https://github.com/liquid-utilities/includes-attribution/) to ensure
I've little, to no, excuse for not keeping attributions up-to date.

> No I don't expect every company to use Liquid, but I also don't believe
> multi-million/billion/trillion dollar companies cannot invest a few dev-hours
> into implementing something similar for their own content management system.


______


## Ethics


My thinking currently is; if I tell them bots not to scrap a path and they
chose to scrape it, a path clearly labeled as _bait_ or similar, then I'm
ethically in the clear and not at all responsible for them doing dumb stuff.

